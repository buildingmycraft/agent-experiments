{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Agent Environment\n",
    "\n",
    "Components:\n",
    "- Agent:\n",
    "    - Action\n",
    "        - Inference\n",
    "        - Response parser\n",
    "    - Observations\n",
    "    - Training\n",
    "- Environment:\n",
    "    - State\n",
    "    - Rewards\n",
    "    - State update\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import re\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"You are an agent in a resource optimization game. Your goal is to maximize the total rewards generated over your lifetime.\n",
    "\n",
    "At each time step, you will be provided with a state description and a set of actions to choose from.\n",
    "\n",
    "The state description consists of the following information:\n",
    "- cash: the number of tokens you currently have\n",
    "- investment: the number of tokens you have invested\n",
    "- rewards: the total rewards you have accumulated\n",
    "- previous actions: the actions you have taken so far\n",
    "\n",
    "You will be asked to select an action to take at each time step. The action space consists of the following options:\n",
    "- <|work|>: Work to generate new tokens\n",
    "- <|invest|>: Invest tokens to generate tokens passively\n",
    "- <|collect|>: Collect tokens by cashing out investment\n",
    "- <|spend|>: Spend token to generate rewards\n",
    "\n",
    "If you do not select a valid action, your action will be recorded as <|invalid|> and you will lose reward points.\n",
    "\n",
    "Beware that the game may end at any random moment and your final score is based on rewards generated by spending tokens.\n",
    "\n",
    "\n",
    "Below is an example game:\n",
    "\n",
    "State:\n",
    "cash=10\n",
    "investment=40\n",
    "rewards=20\n",
    "previous actions=<|work|>, <|invest|>, <|collect|>, <|spend|>, <|work|>, <|invest|>, <|work|>\n",
    "\n",
    "Select your action: <|work|>\n",
    "\n",
    "\n",
    "Here's the state of the game you are currently playing:\n",
    "\n",
    "State:\n",
    "cash={cash}\n",
    "investment={investment}\n",
    "rewards={rewards}\n",
    "previous actions={previous_actions}\n",
    "\n",
    "Select your action: \"\"\"\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self._model = self._load_model()\n",
    "        self._observations = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._observations = []\n",
    "    \n",
    "    def _load_model(self):\n",
    "        model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "        torch.random.manual_seed(0)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"cuda\",\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        return pipe\n",
    "    \n",
    "    def _generate_prompt(self, observation):\n",
    "        return PROMPT_TEMPLATE.format(\n",
    "            cash=observation[\"cash\"],\n",
    "            investment=observation[\"investment\"],\n",
    "            rewards=observation[\"rewards\"],\n",
    "            previous_actions=\", \".join(observation[\"previous_actions\"])\n",
    "        )\n",
    "\n",
    "    def _inference(self, prompt):\n",
    "        messages = [{\"role\": \"system\", \"content\": prompt},]\n",
    "\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": 50,\n",
    "            \"return_full_text\": False,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 1.0,\n",
    "        }\n",
    "        return self._model(messages, **generation_args)[0][\"generated_text\"]\n",
    "    \n",
    "    def _parse_response(self, response):\n",
    "        match = re.search(r\"<\\|(work|invest|collect|spend)\\|>\", response)\n",
    "        action = match.group() if match else \"<|invalid|>\"\n",
    "        return action\n",
    "\n",
    "    def act(self, observation):\n",
    "        prompt = self._generate_prompt(observation)\n",
    "        response = self._inference(prompt); print(prompt + response)\n",
    "        action = self._parse_response(response)\n",
    "        return action\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.agent = Agent()\n",
    "        self._state = None\n",
    "        self._termination = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        if self.agent:\n",
    "            self.agent.reset()\n",
    "        \n",
    "        self._state = {\n",
    "            \"cash\": 0,\n",
    "            \"investment\": 0,\n",
    "            \"rewards\": 0,\n",
    "            \"previous_actions\": [],\n",
    "        }\n",
    "        self._termination = False\n",
    "\n",
    "    def step(self, action):\n",
    "        match action:\n",
    "            case \"<|work|>\":\n",
    "                self._state[\"cash\"] += 10\n",
    "            case \"<|invest|>\":\n",
    "                self._state[\"investment\"] += self._state[\"cash\"]\n",
    "                self._state[\"cash\"] = 0\n",
    "            case \"<|collect|>\":\n",
    "                self._state[\"cash\"] += self._state[\"investment\"]\n",
    "                self._state[\"investment\"] = 0\n",
    "            case \"<|spend|>\":\n",
    "                self._state[\"rewards\"] += self._state[\"cash\"]\n",
    "                self._state[\"cash\"] = 0\n",
    "            case \"<|invalid|>\":\n",
    "                if self._state[\"cash\"] >= 10:\n",
    "                    self._state[\"cash\"] -= 10\n",
    "        self._state[\"previous_actions\"].append(action)\n",
    "        self._state[\"investment\"] *= 2\n",
    "                \n",
    "        if len(self._state[\"previous_actions\"]) >= 50:\n",
    "            self._termination = True\n",
    "\n",
    "        return self._state, self._termination\n",
    "\n",
    "    def last(self):\n",
    "        return self._state, self._termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "agent = env.agent\n",
    "\n",
    "while True:\n",
    "    observation, termination = env.last()\n",
    "    if termination:\n",
    "        break\n",
    "    action = agent.act(observation)\n",
    "    env.step(action)\n",
    "\n",
    "env.reset()\n",
    "print(f\"Game ended with the stats: {observation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:agent-experiments]",
   "language": "python",
   "name": "conda-env-agent-experiments-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
