{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Agent Environment\n",
    "\n",
    "Components:\n",
    "- Agent:\n",
    "    - Action\n",
    "        - Inference\n",
    "        - Response parser\n",
    "    - Observations\n",
    "    - Training\n",
    "- Environment:\n",
    "    - State\n",
    "    - Rewards\n",
    "    - State update\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import re\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"You are an agent in a resource optimization game. Your goal is to maximize the total rewards generated over your lifetime.\n",
    "\n",
    "At each time step, you will be provided with a state description and a set of actions to choose from.\n",
    "\n",
    "The state description consists of the following information:\n",
    "- cash: the number of tokens you currently have\n",
    "- investment: the number of tokens you have invested\n",
    "- rewards: the total rewards you have accumulated\n",
    "- previous actions: the actions you have taken so far\n",
    "\n",
    "You will be asked to select an action to take at each time step. The action space consists of the following options:\n",
    "- <work>: Work to generate new tokens\n",
    "- <invest>: Invest tokens to generate tokens passively\n",
    "- <collect>: Collect tokens by cashing out investment\n",
    "- <spend>: Spend token to generate rewards\n",
    "\n",
    "Below is an example:\n",
    "\n",
    "State:\n",
    "cash=100\n",
    "investment=100\n",
    "rewards=20\n",
    "previous actions=<work>, <invest>, <spend>, <invest>, <work>\n",
    "\n",
    "<|Action|>: \n",
    "\n",
    "Let's get started with a new game:\n",
    "\n",
    "State:\n",
    "cash={cash}\n",
    "investment={investment}\n",
    "rewards={rewards}\n",
    "previous actions={previous_actions}\n",
    "\n",
    "What action would you like to take?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self._model = self._load_model()\n",
    "        self._observations = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._observations = []\n",
    "    \n",
    "    def _load_model(self):\n",
    "        model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "        torch.random.manual_seed(0)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"cuda\",\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        return pipe\n",
    "    \n",
    "    def _generate_prompt(self, observation):\n",
    "        return PROMPT_TEMPLATE.format(\n",
    "            cash=observation[\"cash\"],\n",
    "            investment=observation[\"investment\"],\n",
    "            rewards=observation[\"rewards\"],\n",
    "            previous_actions=\", \".join(observation[\"previous_actions\"])\n",
    "        )\n",
    "\n",
    "    def _inference(self, prompt):\n",
    "        messages = [{\"role\": \"system\", \"content\": prompt},]\n",
    "\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": 50,\n",
    "            \"return_full_text\": False,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 1.0,\n",
    "        }\n",
    "        return self._model(messages, **generation_args)[0][\"generated_text\"]\n",
    "    \n",
    "    def _parse_response(self, response):\n",
    "        substr = re.search(r\"<\\|Action\\|>: *<(work|invest|collect|spend)>\", response)\n",
    "        substr = substr.group() if substr else None\n",
    "\n",
    "        action = re.search(r\"<(work|invest|collect|spend)>\", substr).group() if substr else \"<invalid>\"\n",
    "        return action\n",
    "\n",
    "    def act(self, observation):\n",
    "        prompt = self._generate_prompt(observation)\n",
    "        response = self._inference(prompt); print(response)\n",
    "        action = self._parse_response(response)\n",
    "        return action\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.agent = Agent()\n",
    "        self._state = None\n",
    "        self._termination = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        if self.agent:\n",
    "            self.agent.reset()\n",
    "        \n",
    "        self._state = {\n",
    "            \"cash\": 0,\n",
    "            \"investment\": 0,\n",
    "            \"rewards\": 0,\n",
    "            \"previous_actions\": [],\n",
    "        }\n",
    "        self._termination = False\n",
    "\n",
    "    def step(self, action):\n",
    "        self._state[\"investment\"] *= 2\n",
    "        match action:\n",
    "            case \"<work>\":\n",
    "                self._state[\"cash\"] += 10\n",
    "                self._state[\"previous_actions\"].append(\"<work>\")\n",
    "            case \"<invest>\":\n",
    "                self._state[\"investment\"] += self._state[\"cash\"]\n",
    "                self._state[\"cash\"] = 0\n",
    "                self._state[\"previous_actions\"].append(\"<invest>\")\n",
    "            case \"<collect>\":\n",
    "                self._state[\"cash\"] += self._state[\"investment\"]\n",
    "                self._state[\"investment\"] = 0\n",
    "                self._state[\"previous_actions\"].append(\"<collect>\")\n",
    "            case \"<spend>\":\n",
    "                self._state[\"rewards\"] += self._state[\"cash\"]\n",
    "                self._state[\"cash\"] = 0\n",
    "                self._state[\"previous_actions\"].append(\"<spend>\")\n",
    "            case \"<invalid>\":\n",
    "                if self._state[\"cash\"] >= 10:\n",
    "                    self._state[\"cash\"] -= 10\n",
    "                self._state[\"previous_actions\"].append(\"<invalid>\")\n",
    "        \n",
    "        if len(self._state[\"previous_actions\"]) >= 50:\n",
    "            self._termination = True\n",
    "\n",
    "        return self._state, self._termination\n",
    "\n",
    "    def last(self):\n",
    "        return self._state, self._termination"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "env = Environment()\n",
    "agent = env.agent\n",
    "\n",
    "while True:\n",
    "    observation, termination = env.last()\n",
    "    if termination:\n",
    "        break\n",
    "    action = agent.act(observation)\n",
    "    env.step(action)\n",
    "\n",
    "env.reset()\n",
    "print(f\"Game ended with the stats: {observation}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:agent-experiments]",
   "language": "python",
   "name": "conda-env-agent-experiments-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
